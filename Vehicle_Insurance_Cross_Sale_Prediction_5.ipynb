{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "FJNUwmbgGyua",
        "mDgbUHAGgjLW",
        "4_0_7-oCpUZd",
        "bn_IUdTipZyH",
        "xiyOF9F70UgQ",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AniketGhorpade/Vehicle-Insurance/blob/main/Vehicle_Insurance_Cross_Sale_Prediction_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - \n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Classification\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member -** Aniket K. Ghorpade"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Available data in this project includes deographics(gender,age,region code type), Vehicle age(Vehcile age, damage), Policy (premium, sales channel etc). Our main task in this project was to build a machine learning model which will predict weather customer is going to buy vehicle insurance or not by using above independant variables. It was a supervised machine learning problem and we have to predict a categorical variable that's why we have used classification based machine learning algorithms in this project.\n",
        "\n",
        "Intially we have performed data cleanig and data transformation to bring available data in desirable format. Then we have performed sevral data transformations and derived few new features so that it will be easier to do exploratory analysis. After that we have perfomrmed exploratory data analysis respectively we did univariate analysis, bia-variate analysis to check relationship between different variables followed by multi-variate analysis, so it gave us various insights which are useful to take business decisions. With all above procedure we have finished our exploratory data analysis part of this project.\n",
        "\n",
        "Then we have performed hypothesis testing to check various hypothetical statements by using various statistical methods. \n",
        "\n",
        "After all above procedure we have moved towards machine learning part of our project in which we have made our data model ready by doing various transformations such as label encoding, outlier handlings etc. Then we have selected few machine learning models which are suitable in our case as per the objective of our project. After selecting those 3-4 models we have fitted traning data and tested with our test data. Then did evaluation of those models by using various accuracy metrices and cross validation techniques. Then repeated this process again by doing hyper parameter tunning and in this way we are able to find the best supervised classification machine learning model for above problem statment with tunned hyperparameters."
      ],
      "metadata": {
        "id": "SeJU9S3VGL1A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/AniketGhorpade/Vehicle-Insurance"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our client is an Insurance company that has provided health insurance to it's customers now they need our help in building a model to predict weather the policy holders from past year will also be interested in Vehicle insurance provided by the company. An isurance policy is an arrangment by which a company undartakes to provide a gurantee of compensation for specified loss, damage, illness or death in return for the payment of a specified premium. A premium is a sum of money that the customer needs to pay regularly to a insurance company for this guarantee. For example you may pay a premium of Rs. 5000/- each year for insurance cover of Rs. 2,00,000/- so that , if god forbid, you fall ill and need to be hospitallised in that year, the insurance provider company will bear the cost of hospitalisation etc. for upto rs. 2,00,000. \n",
        "\n",
        "Just like medical insurance, there is a vehicle insurance where every year customers need to pay a certain premium amount to insurance provider company so that in case of unfortunate accident by the vehicle, the insrance company will provide a compensation(called sum assured to the customer).\n",
        "\n",
        "Now in order to predict, weather the customer would be interested in Vehicle insurance, you have information about demographics(gender,age,region code type), Vehicle(Age,Damage), Policy(Premium,sourcing channel) etc."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required. \n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits. \n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule. \n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data manipulation libraries \n",
        "\n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "\n",
        "# Data visualisation libraries\n",
        "\n",
        "import matplotlib.pyplot as plt \n",
        "import seaborn as sns\n",
        "from matplotlib import colors\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore') \n",
        "\n",
        "# Statistics Library\n",
        "import scipy.stats as stats\n",
        "\n",
        "\n",
        "# Machine Learning Libraries\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder,StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split,StratifiedKFold,RepeatedStratifiedKFold,GridSearchCV,RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression,RidgeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score,roc_auc_score, roc_curve, precision_recall_curve, confusion_matrix"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "eVkO5C85BsM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Capstone Projects Submission/3. ML Classification: Health Insurance/TRAIN-HEALTH INSURANCE CROSS SELL PREDICTION.csv')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(5)"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'before: {df[df.duplicated()].shape}') \n",
        "# There are around 0 duplicate values in the dataset dropping them \n",
        "df.drop_duplicates(keep=\"first\", inplace=True)\n",
        "print(f'after: {df[df.duplicated()].shape}')"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No any missing value is present in the dataset"
      ],
      "metadata": {
        "id": "Z2DKdLjUC3qw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   id - Unique Id for customer\n",
        "2.   Gender - Male/Female\n",
        "3. Age - Age of customer\n",
        "4. Driving License - customer has DL or not\n",
        "5. Region_code - unique code for the region of the customer \n",
        "6. Previouly insured - Customer already has vehicle insurance or not\n",
        "7. Vehicle age - Age of the vehicle \n",
        "8. vehicle damage - Past damage present or not \n",
        "9. Annual_Premium - the amount customer needs to pay as premium \n",
        "10. policy sales channel- Anonymized code for the channel of outreaching to the customer i.e. different agents, over mail, over phone, In person etc.\n",
        "11. Vintage - Number of days customer has been associated with the company.\n",
        "12. Response - customer is interested or not."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe(include='all')"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description "
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) id: unique id is there for all the customers.\n",
        "\n",
        "2) gender: has 2 unique id's male and female, around 206089 records belongs to male customers.\n",
        "\n",
        "3) Age: mean age of customers is 39 years with standard daviation 15, it is in the range of 20 to 85, 75 % of the customers are below 49 years, it is clearly showing that age column is positively skewed.\n",
        "\n",
        "4) Driving License: Around 99.9 % customers have driving license, mean is 0.99 for contineous varible consisting only two values 1 and 0.\n",
        "\n",
        "5) Region code: 53 various region codes are there, starting from 0 to 52. Data is distibuted equally for this region codes that is mean is 26.78 which is some what equal to median which is showing normal distributions of region codes.\n",
        "\n",
        "6) Previously insured: From the available data only 45 % peoples were previously insured.\n",
        "\n",
        "7) Vehicle age: 60% of the vehicles are between 1-2 years of age, and they are divided in 3 categories below 1 year, 1-2 year and above 2 years.\n",
        "\n",
        "8) Vehicle Damage: 50 % of the vehcles has previous damage present.\n",
        "\n",
        "9) Annual Premium: It varies from 2630 to 5,30,000 with mean of 30,500 and median of 31,000 it is looking like a normal distribution.\n",
        "\n",
        "10) policy sales channel: 163 various policy sales channels are there.\n",
        "\n",
        "11) Vintage: On an average customers are connceted from 154 days and median is also 154 distribution of vintage is looking normally distributed.\n",
        "\n",
        "12) Response: It is dependant variable. 32 % of the customers has taken vehicle insurance from availble datset."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for col in df.columns:\n",
        "  print(f'{col}: {len(df[col].unique())}')"
      ],
      "metadata": {
        "id": "TqXJz5BoY8Mf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfnew = df.copy()\n",
        "dfnew.head(5)"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Let's change binary categorical columns in 1/0 format to yes/no format\n",
        "cols = ['Driving_License','Previously_Insured','Response']\n",
        "for col in cols:\n",
        "  dfnew[col] = dfnew[col].astype('str')\n",
        "  dfnew[col] = dfnew[col].replace({'1':'Yes','0':'No'})\n",
        "  \n",
        "dfnew.head(5)"
      ],
      "metadata": {
        "id": "TT5DHRZcbX1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Let's change columns in float format to int format which are showing codes and channel numbers \n",
        "cols = ['Region_Code','Policy_Sales_Channel']\n",
        "for col in cols:\n",
        "  dfnew[col] = dfnew[col].astype('int')\n",
        "dfnew.head(5)"
      ],
      "metadata": {
        "id": "NobGi9UKiVXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1) Changed binary categorical columns available in encoded format into there original formats. \n",
        "##### 2) Changed data type of columns which are showing region codes, policy sales channel code into integer format"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Univariate Analysis**"
      ],
      "metadata": {
        "id": "-WVzYVAjoHwN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Distribution of categorical variables**"
      ],
      "metadata": {
        "id": "tTpTRIXPp9hK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def countplot(df,x,figsize):\n",
        "\n",
        "  ''' This function returns barplot of value counts of categorical variables'''\n",
        "\n",
        "  plt.figure(figsize=figsize)\n",
        "\n",
        "  sns.set(style=\"whitegrid\", color_codes=True)\n",
        "\n",
        "  plots = sns.countplot(x=df[x])\n",
        "\n",
        "  for bar in plots.patches:\n",
        "      plots.annotate(format(bar.get_height(), '.0f'),\n",
        "                    (bar.get_x() + bar.get_width() / 2,\n",
        "                      bar.get_height()), ha='center', va='center',\n",
        "                    size=14, xytext=(0, 8),\n",
        "                    textcoords='offset points')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Distribution of data points w.r.t. gender.** "
      ],
      "metadata": {
        "id": "Px7v2aguqIpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "countplot(df=dfnew,x='Gender',figsize=(10,5))"
      ],
      "metadata": {
        "id": "TTh0RqWvpa8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. In available data points 53% records belongs male clients and 47% records belongs to female clients.\n",
        "2. Data doesn't contain much biasness w.r.t gender."
      ],
      "metadata": {
        "id": "2_r9VSWlqqLy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. How many clients are legally authorised to drive the vehicle ?** "
      ],
      "metadata": {
        "id": "sZw0QWiHrob9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive = pd.DataFrame(dfnew.Driving_License.value_counts().reset_index())\n",
        "drive.rename(columns = {'index':'License','Driving_License':'Total Counts'}, inplace = True)\n",
        "\n",
        "## Pie Chart\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "data = drive['Total Counts']\n",
        "keys = drive['License']\n",
        "  \n",
        "# declaring exploding pie\n",
        "explode = [0.01,0.01]\n",
        "# define Seaborn color palette to use\n",
        "palette_color = sns.color_palette('Dark2')\n",
        "  \n",
        "# plotting data on chart\n",
        "plt.pie(data, labels=['Have License','Does Not Have License'], colors='GY',explode=explode,autopct='%.00f%%',textprops={'fontsize': 14})\n",
        "\n",
        "\n",
        "plt.legend(fontsize=14)\n",
        "\n",
        "plt.title(\"Drivers with/without License\",size=16)\n",
        "# displaying chart\n",
        "plt.show() "
      ],
      "metadata": {
        "id": "E_tQ3OtXtJ0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. In available data points 99.9% clients has proper driving license.\n",
        "2. It is a very good thing that whatever clients that are connected with the company are leagally authorised to drive in there respective regions."
      ],
      "metadata": {
        "id": "cixrg5-SuGSP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. How many customers has bought insurance for there vehicles in past ?** "
      ],
      "metadata": {
        "id": "EfU5wNsdvD5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "countplot(df=dfnew,x='Previously_Insured',figsize=(10,5))"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. In available data points 47% clients has bought insurance in past and 53% clients hasn't bought insurance in past.\n",
        "2. Data doesn't contain much biasness w.r.t previous vehicle insurance buy's by clients."
      ],
      "metadata": {
        "id": "g-OuTQs31PoH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Age of vehicles** "
      ],
      "metadata": {
        "id": "97NZ6bGZvCPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "age = pd.DataFrame(dfnew.Vehicle_Age.value_counts().reset_index())\n",
        "age.rename(columns = {'index':'Vehical_Age','Vehicle_Age':'Total_Vehicals'}, inplace = True)\n",
        "\n",
        "## Pie Chart\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "data = age['Total_Vehicals']\n",
        "keys = age['Vehical_Age']\n",
        "  \n",
        "# declaring exploding pie\n",
        "explode = [0.01,0.01,0.2]\n",
        "# define Seaborn color palette to use\n",
        "palette_color = sns.color_palette('coolwarm_r')\n",
        "  \n",
        "# plotting data on chart\n",
        "plt.pie(data, labels=['1-2 years','Less than year','More than 2 years'], colors=palette_color,explode=explode,autopct='%.0f%%',textprops={'fontsize': 14})\n",
        "\n",
        "\n",
        "plt.legend(fontsize=14)\n",
        "\n",
        "plt.title(\"Count of vehicles w.r.t there age\",size=16)\n",
        "# displaying chart\n",
        "plt.show() "
      ],
      "metadata": {
        "id": "rx8nafVfUV7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. In available data points 43% clients has bought vehicle before 1 year,53 % of vehicles age is between 1-2 years and only 4% vehicles has age more than 2 years.\n",
        "2. Data has information about the vehicles which are comparatively newer and recently bought."
      ],
      "metadata": {
        "id": "SEkauhsFWoZu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. How many vehicles in the dataset has previous history of damage ?** "
      ],
      "metadata": {
        "id": "yK1TM2J1XYVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "countplot(df=dfnew,x='Vehicle_Damage',figsize=(10,5))"
      ],
      "metadata": {
        "id": "RoqANHshXlW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. In available data points 51% vehicles has previous history of damage and 49% vehicles doesn't has any history of damage.\n",
        "2. Data has information about the vehicles equally distributed from the point of view damage/accident history."
      ],
      "metadata": {
        "id": "XMOk1uQRX7lg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Top Policy Sales Channel** "
      ],
      "metadata": {
        "id": "WiRCW1OcY2Ws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy_sales = pd.DataFrame(dfnew.Policy_Sales_Channel.value_counts().reset_index())\n",
        "policy_sales.rename(columns = {'index':'Policy_Sales_Channel','Policy_Sales_Channel':'Total_Customers'}, inplace = True)\n",
        "#policy_sales.head(10)"
      ],
      "metadata": {
        "id": "KVLKZTTLZI8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "sns.set(style=\"whitegrid\", color_codes=True)\n",
        "plots = sns.barplot(x='Policy_Sales_Channel', y='Total_Customers', data=policy_sales.head(10),color='deepskyblue')\n",
        "plots.set_xticklabels(plots.get_xticklabels(), rotation=1, ha=\"right\",size=12)\n",
        "\n",
        "\n",
        " \n",
        "# Iterating over the bars one-by-one\n",
        "for bar in plots.patches:\n",
        "    plots.annotate(format(bar.get_height(), '.0f'),\n",
        "                   (bar.get_x() + bar.get_width() / 2,\n",
        "                    bar.get_height()), ha='center', va='center',\n",
        "                   size=14, xytext=(0, 8),\n",
        "                   textcoords='offset points')\n",
        " \n",
        "# Setting the label for x-axis\n",
        "plt.xlabel(\"Policy_Sales_Channel\", size=14)\n",
        " \n",
        "# Setting the label for y-axis\n",
        "plt.ylabel(\"Total Customers\", size=14)\n",
        " \n",
        "# Setting the title for the graph\n",
        "plt.title(\"Top Policy_Sales_Channels\",size=16)\n",
        " \n",
        "# Finally showing the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3r0zVD2laAVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. In above graph we could see that around 26% of the customers belongs to sales channel 152, followed by channel 26 around 13% and channel 124 around 12%.\n",
        "2. From 155 various channels around 75 % of customers are connected with 10 channels which are mentioned above. \n",
        "3. Which is clearly showing that few channels are contributing more and few channels are comparitively contributing nothing, so there is need of distribution of this traffic into various sales channels, so that burden and dependancy on one particular channel should be minimised."
      ],
      "metadata": {
        "id": "IGcNrfbicdAb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Distribution of customers w.r.t various regions** "
      ],
      "metadata": {
        "id": "-_mLf-4XeR-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regiondf = pd.DataFrame(dfnew.Region_Code.value_counts().reset_index())\n",
        "regiondf.rename(columns = {'index':'RegionCode','Region_Code':'Total_Customers'}, inplace = True)\n",
        "#regiondf.head(20)"
      ],
      "metadata": {
        "id": "5fQptGpveo2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "sns.set(style=\"whitegrid\", color_codes=True)\n",
        "plots = sns.barplot(x='RegionCode', y='Total_Customers', data=regiondf.head(15),color='deepskyblue')\n",
        "plots.set_xticklabels(plots.get_xticklabels(), rotation=20, ha=\"right\",size=12)\n",
        "\n",
        "\n",
        " \n",
        "# Iterating over the bars one-by-one\n",
        "for bar in plots.patches:\n",
        "    plots.annotate(format(bar.get_height(), '.0f'),\n",
        "                   (bar.get_x() + bar.get_width() / 2,\n",
        "                    bar.get_height()), ha='center', va='center',\n",
        "                   size=10, xytext=(0, 8),\n",
        "                   textcoords='offset points')\n",
        " \n",
        "# Setting the label for x-axis\n",
        "plt.xlabel(\"Region Code\", size=14)\n",
        " \n",
        "# Setting the label for y-axis\n",
        "plt.ylabel(\"Total Customers\", size=14)\n",
        " \n",
        "# Setting the title for the graph\n",
        "plt.title(\"Customers w.r.t Regions\",size=16)\n",
        " \n",
        "# Finally showing the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dXvPZoOUfUbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Region code 28 is contributing highest as it has around 25% customers, followed by Region 8 around 11% and region 41 and 47 6% each.\n",
        "2. There are regions with very few customers need to concentrate in those regions, so that more clients shoul be connected with us from those regions."
      ],
      "metadata": {
        "id": "QDh55DFhgbzJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Distribution of contineous variables**"
      ],
      "metadata": {
        "id": "yt1Z7JWHhkHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def distr(DF,x,n_bins=10):\n",
        "\n",
        "  ''' This function gives univariate distribution of contineous variable in the form of Histogram '''\n",
        "\n",
        "  legend = ['distribution']\n",
        "\n",
        "  # Creating histogram\n",
        "  fig, axs = plt.subplots(1, 1,\n",
        "                          figsize =(8,5),\n",
        "                          tight_layout = True)\n",
        "  \n",
        "  \n",
        "  # Remove axes splines\n",
        "  for s in ['top', 'bottom', 'left', 'right']:\n",
        "      axs.spines[s].set_visible(False)\n",
        "  \n",
        "  # Remove x, y ticks\n",
        "  axs.xaxis.set_ticks_position('none')\n",
        "  axs.yaxis.set_ticks_position('none')\n",
        "    \n",
        "  # Add padding between axes and labels\n",
        "  axs.xaxis.set_tick_params(pad = 5)\n",
        "  axs.yaxis.set_tick_params(pad = 10)\n",
        "  \n",
        "  # Add x, y gridlines\n",
        "  axs.grid(b = True, color ='grey',\n",
        "          linestyle ='-.', linewidth = 0.5,\n",
        "          alpha = 0.6)\n",
        "  kur = round(DF[x].kurt(),2)\n",
        "  ske = round(DF[x].skew(),2)\n",
        "  mean =  round(DF[x].mean(),2)\n",
        "  median =  round(DF[x].median(),2)\n",
        "  # Add Text watermark\n",
        "  fig.text(0.9, 0.75, 'Kurtosis: '+str(kur)+' & Skewness: '+str(ske),fontsize = 14,color ='black',ha ='right', va ='bottom',alpha = 1.0)\n",
        "  fig.text(0.9, 0.68, 'Mean: '+str(mean)+' & Median: '+str(median),fontsize = 14,color ='black',ha ='right', va ='bottom',alpha = 1.0)\n",
        "  \n",
        "  # Creating histogram\n",
        "  N, bins, patches = axs.hist(DF[x], bins = n_bins)\n",
        "  \n",
        "  # Setting color\n",
        "  fracs = ((N**(1 / 5)) / N.max())\n",
        "  norm = colors.Normalize(fracs.min(), fracs.max())\n",
        "  \n",
        "  for thisfrac, thispatch in zip(fracs, patches):\n",
        "      color = plt.cm.viridis(norm(thisfrac))\n",
        "      thispatch.set_facecolor(color)\n",
        "  \n",
        "  # Adding extra features   \n",
        "  plt.xlabel(x,size=14)\n",
        "  plt.ylabel(\"Count\",size=14)\n",
        "  plt.legend(legend)\n",
        "  plt.title('Distribution Of '+x,size=16)\n",
        "  \n",
        "  # Show plot\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "Gm_ko739i4PH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfnew.columns"
      ],
      "metadata": {
        "id": "gUh700qCjckw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Distribution of age of customers** "
      ],
      "metadata": {
        "id": "5bIoXavBla2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distr(DF=dfnew,x='Age',n_bins=20)"
      ],
      "metadata": {
        "id": "wE5JIZJ5jOR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Distribution of age is mesokurtic with slight positive skewness, mean is not bad and it is somewhat near to median.\n",
        "2. 75 % of the clients are below 48 years of age which is good thing.\n",
        "3. Mean and median both are representing age of customers connected with company in this case.\n",
        "4. Around 35% of the clients have age in range from 20-26 that means so many young clients are connected with us, which is a positive thing from business expansion and youth connectivity point of view."
      ],
      "metadata": {
        "id": "9PWDlF-LlmPs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. Distribution of Annual Premium that is promised to the customers by company**"
      ],
      "metadata": {
        "id": "5fdUe3irnjCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distr(DF=dfnew,x='Annual_Premium',n_bins=200)"
      ],
      "metadata": {
        "id": "WKBTvSR1lkTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Distribution of annual premium is highly positively skewed but it is so highly leptokurtic with kurtosis of 34, so mean is definately representing data but due to few outliers distribution is getting affected.\n",
        "2. We could replace outliers with mean or median value or upper whicker value as we could see in graph also outliers are present in right side.\n",
        "3. We will treat this variable at the time of treating outliers and we will try to make it's distribution normal by applying outlier treatment processes."
      ],
      "metadata": {
        "id": "nmmJ-olGqEk3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10. Distribution of vintage time from which clients are connected with company**"
      ],
      "metadata": {
        "id": "ZeI_0jPGoAXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distr(DF=dfnew,x='Vintage',n_bins=5)"
      ],
      "metadata": {
        "id": "HHXPkKrKrntR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Distribution of vintage is equally distributed throughout all the bins that are present and we could say infomration is availble for all type of clients which are connected from 10 days to 300 days.\n",
        "2. Mean and median are almost equal and no any peakedness is there distribution is platykurtic, apart from that no skewness is there skewness is also 0."
      ],
      "metadata": {
        "id": "H7v4G88Jsfmz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Relation between independant variables and response**"
      ],
      "metadata": {
        "id": "AHFuUmsrKP3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(3)"
      ],
      "metadata": {
        "id": "izjVjq6hJmmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**11. Is response of clients depend upon gender ?**"
      ],
      "metadata": {
        "id": "S2RLh5pwSn6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gender_reln = pd.DataFrame(df.groupby(by='Response')['Gender'].value_counts())\n",
        "gender_reln.rename(columns ={'Gender':'Counts'},inplace=True)\n",
        "gender_reln.reset_index(inplace=True)"
      ],
      "metadata": {
        "id": "8DJWqKm0LcwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = ['Male','Female']\n",
        "yyes = [28525,18185]\n",
        "Zno = [177564,156835]\n",
        "  \n",
        "X_axis = np.arange(len(X))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "\n",
        "plt.bar(X_axis - 0.2, yyes, 0.4, label = 'Yes')\n",
        "plt.bar(X_axis + 0.2, Zno, 0.4, label = 'No')\n",
        "\n",
        "\n",
        "plt.xticks(X_axis, X)\n",
        "plt.xlabel(\"Gender\",size=14)\n",
        "plt.ylabel(\"Number of Customers\",size=14)\n",
        "plt.title(\"Response of customers w.r.t there genders\",size=16)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-ZVcvD_VMrn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Conversion rate for male customers is 14 % and for female customers it is 10 %, which is very low.\n",
        "2. Gender is not affecting conversion rate, hence we could say that conversion is not depend upon the gender of a customer."
      ],
      "metadata": {
        "id": "3x4hRV0LT6Q5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**12. Is response of clients depend upon age of vehicle they are using ?**"
      ],
      "metadata": {
        "id": "cEieTAJyQ9H_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "veh_age = pd.DataFrame(df.groupby(by='Response')['Vehicle_Age'].value_counts())\n",
        "veh_age.rename(columns ={'Vehicle_Age':'Counts'},inplace=True)\n",
        "veh_age.reset_index(inplace=True)"
      ],
      "metadata": {
        "id": "pk__5vvgRLJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = ['< 1 Year','1-2 Year','> 2 Year']\n",
        "yyes = [7202,34806,4702]\n",
        "Zno = [157584,165510,11305]\n",
        "  \n",
        "X_axis = np.arange(len(X))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "\n",
        "plt.bar(X_axis - 0.2, yyes, 0.4, label = 'Yes')\n",
        "plt.bar(X_axis + 0.2, Zno, 0.4, label = 'No')\n",
        "\n",
        "\n",
        "plt.xticks(X_axis, X)\n",
        "plt.xlabel(\"Vehicle Age\",size=14)\n",
        "plt.ylabel(\"Number of Customers\",size=14)\n",
        "plt.title(\"Response of customers w.r.t age of there vehicle\",size=16)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4b87PlSmRuDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. For vehciles of age less than a year only 4% of clients has responded positively, for vehicles with age 1-2 years 17% clients has responded positively, for vehicles with age greater than 2 year 29% of clients has responded positively.\n",
        "2. From the above numbers we could conclude that as age of vehicle is increasing there are more chances that owners of vehicles will agree to insure there vehicle."
      ],
      "metadata": {
        "id": "3nmBaHLfTK4d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**13. Is previous history of vehicle damage affecting response of clients ?**"
      ],
      "metadata": {
        "id": "zwDOfb6lVCvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "veh_damage = pd.DataFrame(df.groupby(by='Response')['Vehicle_Damage'].value_counts())\n",
        "veh_damage.rename(columns ={'Vehicle_Damage':'Counts'},inplace=True)\n",
        "veh_damage.reset_index(inplace=True)"
      ],
      "metadata": {
        "id": "64C1nduUVXZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = ['No','Yes']\n",
        "yyes = [982,45728]\n",
        "Zno = [187714,146685]\n",
        "  \n",
        "X_axis = np.arange(len(X))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "\n",
        "plt.bar(X_axis - 0.2, yyes, 0.4, label = 'Yes')\n",
        "plt.bar(X_axis + 0.2, Zno, 0.4, label = 'No')\n",
        "\n",
        "\n",
        "plt.xticks(X_axis, X)\n",
        "plt.xlabel(\"Vehicle Damage\",size=14)\n",
        "plt.ylabel(\"Number of Customers\",size=14)\n",
        "plt.title(\"Response of customers w.r.t Vehicle damage\",size=16)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OFtRDcXpVxu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. From above graph we could easily conclude that clients who has vehicles with damage history will respond in postive way to the vehicle insurance as around 24% of hte clients with vehicle damage has responded yes.\n",
        "2. Clients with no vehicle damage history has this rate of only 0.008% which is very very low."
      ],
      "metadata": {
        "id": "S83e0QU-Wabb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Relation between contineous independant variables**"
      ],
      "metadata": {
        "id": "VLXUo_HmZrBP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**14. Is Age of a client is affecting Annual_Premium ?**"
      ],
      "metadata": {
        "id": "bikco1a5Z0TM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# draw regplot\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.regplot(x = \"Age\", y = \"Annual_Premium\",data = df,fit_reg=True,dropna = True,color='red').set_title('Relation between Age and Annual Premium',size=14)\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2xHMxrqPaIKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above graph we could conclude that there is no any relationship between Age and annual premium that is being provided to clients. "
      ],
      "metadata": {
        "id": "P8O71aL9dA-o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**15. Is Vintage of a client is affecting Annual_Premium ?**"
      ],
      "metadata": {
        "id": "affGOXvadXV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# draw regplot\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.regplot(x = \"Vintage\", y = \"Annual_Premium\",data = df,fit_reg=True,dropna = True,color='red').set_title('Relation between Vintage and Annual Premium',size=14)\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Y_RTLPbedmu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above graph we could conclude that there is no any relationship between Vintage and annual premium that is being provided to clients. "
      ],
      "metadata": {
        "id": "mN_AGi_Bd9X7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**16. Correlation Heatmap**"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "col_le = ['Gender','Vehicle_Age','Vehicle_Damage']\n",
        "le = LabelEncoder()\n",
        "for cols in col_le:\n",
        "  df[cols] = le.fit_transform(df[cols])\n",
        "df.head(3)"
      ],
      "metadata": {
        "id": "nPzrxVFIYiDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop('id',axis=1) "
      ],
      "metadata": {
        "id": "iUK4M6YbeMns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (20,10))\n",
        "colormap = plt.cm.plasma\n",
        "sns.heatmap(df.corr(),cmap = colormap,annot=True);\n",
        "sns.set(font_scale=1.1)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BmpesjfYeWkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Age and gender has 15 % of weak positive correlation.\n",
        "2. Age and insurance history has 25% of moderate negative correlation. Which is showing that with increase in age peoples are not much interested in vehicle insurance.\n",
        "3. Vehicle age and client age has strong negative correlation of around 52%. This two features are affecting each other inversly.\n",
        "4. Vehicle age and isurance history has a weak positive relation of 17%.\n",
        "5. Vehicle Damage and age of client has 27% of moderate positive correlation. \n",
        "6. Previously insured and vehicle damage both columns has strong negative correaltion of around 82%.\n",
        "7. Vehicle damage and vehicle age has weak negative corrlation of around 17%.\n",
        "8. Policy sales channel and age has storng negative correlation of around 58%.\n",
        "9. Policy sales channel and previously insured has moderate positive relation of around 22%.\n",
        "10. Policy sales channel and vehicle age has moderate positive relation of around 39%.\n",
        "11. Policy sales channel and vehicle damage has moderate negative relation of around 22%.\n",
        "12. Age and response has 11% of weak positive correlation.\n",
        "13. Resposne and previously insured has 34% of moderate negative correlation.\n",
        "14. Resposne and vehicle damage has 35% of moderate positive correlation."
      ],
      "metadata": {
        "id": "kXD3tnz7fYFn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**17. Pair Plot**"
      ],
      "metadata": {
        "id": "mib0ILpcyh_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (20,10))\n",
        "sns.set(font_scale=0.8)\n",
        "\n",
        "sns.pairplot(df, hue ='Response')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. From above pair plot we could say that data points are heavily mixed with each other, and they are not linearly seprable so, machine learning models with geometrical intution such as support vector machines could not be able to genralise well in this case.\n",
        "2. Same case should be there with logistic regression as it tries to seprate categorical variables linearly by forming s-shape curve it will be difficult for logistic regression also, to genralise well.\n",
        "3. Let's check for linear regression and decision trees in model implementation, pairplots are giving hint's that decision trees or distance based algorithms like KNN may perform better on above dataset as compared to logistic regression or ridge classification."
      ],
      "metadata": {
        "id": "KJWYAgPszOi_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothesis 1: Old clients are more often to buy insurance as compared to young clients."
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "H0 : mean age of clients whom bought vehicle insurance >= mean age of clients whom doesn't bought vehicle insurance.\n",
        "\n",
        "H1 : mean age of clients whom bought vehicle insurance < mean age of clients whom doesn't bought vehicle insurance."
      ],
      "metadata": {
        "id": "U9U_gvZCGiMJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### We will be performing t-test to do hypothesis testing in this case as we have to check distribution for two various categories and we have one catgorical variable and one contineous variable, so t-test will be the best option in this case."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Let's create two different samples of Age column w.r.t two different responses that is our categorical variable 'Yes' or 'No'\n",
        "\n",
        "age_yes = df[df['Response']==1]['Age']\n",
        "age_no = df[df['Response']==0]['Age']"
      ],
      "metadata": {
        "id": "8c0InrMDM7ni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import ttest_ind  \n",
        "    \n",
        "def t_test(x,y,alternative='both-sided'):\n",
        "        _, double_p = ttest_ind(x,y,equal_var = False)\n",
        "        if alternative == 'both-sided':\n",
        "            pval = double_p\n",
        "        elif alternative == 'greater':\n",
        "            if np.mean(x) > np.mean(y):\n",
        "                 pval = double_p/2.\n",
        "            else:\n",
        "                 pval = 1.0 - double_p/2.\n",
        "        elif alternative == 'less':\n",
        "            if np.mean(x) < np.mean(y):\n",
        "                 pval = double_p/2.\n",
        "            else:\n",
        "              pval = 1.0 - double_p/2.\n",
        "\n",
        "        op = 'Hence we are failed to reject null hypothesis (H0) for significane level 0.05'\n",
        "        if pval < 0.05:\n",
        "          op = 'Hence we are rejecting null hypothesis (H0) for significane level 0.05 '\n",
        "        return (f'P-Value: {pval}, {op}')"
      ],
      "metadata": {
        "id": "Ur_CETabUc6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t_test(x=age_yes,y=age_no,alternative='less')"
      ],
      "metadata": {
        "id": "CCISE-B1UkjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hypothesis 1 Conclusion: Mean age of clients whom bought vehicle insurance is more than mean age of clients whom doesn't bought vehicle insurance**"
      ],
      "metadata": {
        "id": "MIQ7STDAkxVR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothesis 2: Vehicle_Age is affecting response of client."
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "H0 : 'Vehicle Age' and 'Response' column has no relationship.\n",
        "\n",
        "H1 : 'Vehicle Age' and 'Response' column has relationship."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- As we are analysing two categorical variables, here We are going to use chi-squre test of independance."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new = pd.crosstab(df.Response,df.Vehicle_Age)\n",
        "new = pd.DataFrame(new)\n",
        "new.reset_index(inplace=True)\n",
        "new['Total'] = new[1] + new[2]\n",
        "new = new.iloc[:,1:]\n",
        "new.loc['Total'] = new.iloc[:, :].sum()\n",
        "new"
      ],
      "metadata": {
        "id": "pY2iYlrzeFwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# significance level\n",
        "alpha = 0.05\n",
        "\n",
        "# Calcualtion of Chisquare\n",
        "chi_square = 0\n",
        "rows = new.index.unique()\n",
        "columns = new.columns.unique()\n",
        "for i in columns:\n",
        "    for j in rows:\n",
        "        O = new[i][j]\n",
        "        E = new[i]['Total'] * new['Total'][j] / new['Total']['Total']\n",
        "        chi_square += (O-E)**2/E"
      ],
      "metadata": {
        "id": "MjDTcHRSfP4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The p-value approach\n",
        "print(\"Approach 1: The p-value approach to hypothesis testing in the decision rule\")\n",
        "p_value = 1 - stats.chi2.cdf(chi_square, (len(rows)-1)*(len(columns)-1))\n",
        "conclusion = \"Failed to reject the null hypothesis.\"\n",
        "if p_value <= alpha:\n",
        "    conclusion = \"Null Hypothesis is rejected.\"\n",
        "        \n",
        "print(\"chisquare-score is:\", chi_square, \" and p value is:\", p_value)\n",
        "print(conclusion)"
      ],
      "metadata": {
        "id": "JyGgIx_UfVOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hypothesis 2 Conclusion: Vehicle Age has relation with the response of client.**"
      ],
      "metadata": {
        "id": "R7dZJJ_8fptV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothesis 3: Amount of premium is affecting response of clients."
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "H0 : mean premium amount of clients whom bought vehicle insurance > mean premium amount of clients whom doesn't bought vehicle insurance.\n",
        "\n",
        "H1 : mean premium amount of clients whom bought vehicle insurance <= mean premium amount of clients whom doesn't bought vehicle insurance."
      ],
      "metadata": {
        "id": "yq2n99Fnh64s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### We will be performing t-test to do hypothesis testing in this case as we have to check distribution for two various categories and we have one catgorical variable and one contineous variable, so t-test will be the best option in this case."
      ],
      "metadata": {
        "id": "ppCHSt92iDvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Let's create two different samples of Annual Premium column w.r.t two different responses that is our categorical variable 'Yes' or 'No'\n",
        "\n",
        "pre_yes = df[df['Response']==1]['Annual_Premium']\n",
        "pre_no = df[df['Response']==0]['Annual_Premium']"
      ],
      "metadata": {
        "id": "AlGbYdvwiKTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t_test(x=pre_yes,y=pre_no,alternative='less')"
      ],
      "metadata": {
        "id": "Ew-HlVJoiluH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hypothesis 3 Conclusion: Mean premium amount of clients whom bought vehicle insurance > Mean premium amount of clients whom doesn't bought vehicle insurance.**"
      ],
      "metadata": {
        "id": "I5jComVGkxwJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We don't have any missing values, present in our dataset**"
      ],
      "metadata": {
        "id": "pQ--iHdImgNK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Region_Code'] = df['Region_Code'].astype('int')\n",
        "df['Annual_Premium'] = df['Annual_Premium'].astype('int')\n",
        "df['Policy_Sales_Channel'] = df['Policy_Sales_Channel'].astype('int')"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(3)"
      ],
      "metadata": {
        "id": "Bg0GcuJBnW8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####**Let's check below variables for any outliers present in them by, visualising boxplot.**"
      ],
      "metadata": {
        "id": "4RmXiXXNKc9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def outlier_detection(df,x):\n",
        "\n",
        "  '''this function gives lower whicker and upper whisker of the feature by using IQR method'''\n",
        "\n",
        "  q1 = df[x].quantile(0.25)\n",
        "  q3 = df[x].quantile(0.75)\n",
        "  iqr = q3-q1\n",
        "  lw = q1 - (1.5*iqr)\n",
        "  uw = q3 + (1.5*iqr)\n",
        "  return(lw,uw)"
      ],
      "metadata": {
        "id": "iKAXwQk2oWrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def outlier_per(df,x):\n",
        "\n",
        "  '''this function gives number of outliers present in the feature by using IQR method'''\n",
        "\n",
        "  q1 = df[x].quantile(0.25)\n",
        "  q3 = df[x].quantile(0.75)\n",
        "  iqr = q3-q1\n",
        "  lw = q1 - (1.5*iqr)\n",
        "  uw = q3 + (1.5*iqr)\n",
        "\n",
        "  cnt = 0\n",
        "  for i in df[x]:\n",
        "    if (i<lw)| (i>uw):\n",
        "      cnt+=1\n",
        "\n",
        "  strng = f'{round(cnt*100/len(df),2)} % Outliers present in column {x} in number it is {cnt}'\n",
        "  strng2 = f'mean: {round(df[x].mean(),2)}'\n",
        "  strng3 = f'median: {round(df[x].median(),2)}'\n",
        "  strng4 = f'minimum: {round(df[x].min(),2)}'\n",
        "  strng5 = f'maximum: {round(df[x].max(),2)}'\n",
        "  strng6 = f'kurtosis: {round(df[x].kurt(),2)}'\n",
        "  strng7 = f'skewness: {round(df[x].skew(),2)}'\n",
        "  \n",
        "  return(strng,strng2,strng3,strng4,strng5,strng6,strng7)"
      ],
      "metadata": {
        "id": "Iv8oK3v7oDAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def outlier_replace(df,x):\n",
        "\n",
        "  '''this function replaces outliers lower than lower whisker with lower whicker and outliers higher than upper whisker with upper whicker'''\n",
        "\n",
        "  lw,uw = outlier_detection(df,x)\n",
        "  lstn = []\n",
        "  for i in df[x]:\n",
        "    if (i<lw):\n",
        "      lstn.append(lw)\n",
        "    if (i>uw):\n",
        "      lstn.append(uw)\n",
        "    else:\n",
        "      lstn.append(i)\n",
        "\n",
        "  df[x+'_out_replace'] = lstn"
      ],
      "metadata": {
        "id": "pVWJVvsgoZle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def boxplot_out(df,X,Y):\n",
        "\n",
        "    '''this function gives boxplot of variables with and without outliers'''\n",
        "\n",
        "    ## detecting outliers and calculating there %\n",
        "\n",
        "    strng0 = f'Boxplot of {X}'\n",
        "    strng = outlier_per(df,X)[0]\n",
        "    strng2 = outlier_per(df,X)[1]\n",
        "    strng3 = outlier_per(df,X)[2]\n",
        "    strng4 = outlier_per(df,X)[3]\n",
        "    strng5 = outlier_per(df,X)[4]\n",
        "    strng6 = outlier_per(df,X)[5]\n",
        "    strng7 = outlier_per(df,X)[6]\n",
        "    \n",
        "\n",
        "    ## plotting figure\n",
        "    fig, axes = plt.subplots(1,2, figsize=(22,6))\n",
        "    sns.boxplot(ax=axes[0], data=df,x=X)\n",
        "\n",
        "    ##labels\n",
        "\n",
        "    fig.text(0.35, 0.9,strng0,fontsize = 16,color ='black',ha ='right',va ='bottom',alpha = 1.0)\n",
        "    fig.text(0.45, 0.75,strng,fontsize = 12,color ='black',ha ='right',va ='bottom',alpha = 1.0)\n",
        "    fig.text(0.45, 0.70,strng2,fontsize = 12,color ='black',ha ='right',va ='bottom',alpha = 1.0)\n",
        "    fig.text(0.45, 0.65,strng3,fontsize = 12,color ='black',ha ='right',va ='bottom',alpha = 1.0)\n",
        "    fig.text(0.45, 0.60,strng4,fontsize = 12,color ='black',ha ='right',va ='bottom',alpha = 1.0)\n",
        "    fig.text(0.45, 0.55,strng5,fontsize = 12,color ='black',ha ='right',va ='bottom',alpha = 1.0)\n",
        "    fig.text(0.45, 0.50,strng6,fontsize = 12,color ='black',ha ='right',va ='bottom',alpha = 1.0)\n",
        "    fig.text(0.45, 0.45,strng7,fontsize = 12,color ='black',ha ='right',va ='bottom',alpha = 1.0)\n",
        "\n",
        "    strng0 = f'Boxplot of {Y}'\n",
        "    strng = outlier_per(df,Y)[0]\n",
        "    strng2 = outlier_per(df,Y)[1]\n",
        "    strng3 = outlier_per(df,Y)[2]\n",
        "    strng4 = outlier_per(df,Y)[3]\n",
        "    strng5 = outlier_per(df,Y)[4]\n",
        "    strng6 = outlier_per(df,Y)[5]\n",
        "    strng7 = outlier_per(df,Y)[6]\n",
        "    \n",
        "\n",
        "    ## plotting figure\n",
        "    # fig, axes = plt.subplots(1,2, figsize=(22,6))\n",
        "    sns.boxplot(ax=axes[1], data=df,x=Y)\n",
        "\n",
        "    ##labels\n",
        "    # plt.title(f'Boxplot of {Y}',size=14)\n",
        "\n",
        "    fig.text(0.80, 0.9,strng0,fontsize = 16,color ='black',ha ='right',va ='bottom',alpha = 1.0)\n",
        "    fig.text(0.85, 0.75,strng,fontsize = 12,color ='black',ha ='right',va ='bottom',alpha = 1.0)\n",
        "    fig.text(0.85, 0.70,strng2,fontsize = 12,color ='black',ha ='right',va ='bottom',alpha = 1.0)\n",
        "    fig.text(0.85, 0.65,strng3,fontsize = 12,color ='black',ha ='right',va ='bottom',alpha = 1.0)\n",
        "    fig.text(0.85, 0.60,strng4,fontsize = 12,color ='black',ha ='right',va ='bottom',alpha = 1.0)\n",
        "    fig.text(0.85, 0.55,strng5,fontsize = 12,color ='black',ha ='right',va ='bottom',alpha = 1.0)\n",
        "    fig.text(0.85, 0.50,strng6,fontsize = 12,color ='black',ha ='right',va ='bottom',alpha = 1.0)\n",
        "    fig.text(0.85, 0.45,strng7,fontsize = 12,color ='black',ha ='right',va ='bottom',alpha = 1.0)\n",
        "    \n",
        "    # show plot\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "u3R5oKoUqJQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfcont = df[['Age','Annual_Premium','Vintage']]\n",
        "outlier_replace(df=dfcont,x='Annual_Premium')\n",
        "dfcont.head(3)"
      ],
      "metadata": {
        "id": "wp01zeeMozDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "boxplot_out(df=dfcont,X='Vintage',Y='Vintage')"
      ],
      "metadata": {
        "id": "YsLJAfZNqHUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "boxplot_out(df=dfcont,X='Age',Y='Age')"
      ],
      "metadata": {
        "id": "rju-WgjM4aX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "boxplot_out(df=dfcont,X='Annual_Premium',Y='Annual_Premium_out_replace')"
      ],
      "metadata": {
        "id": "G0QWKRuY4iZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(3)"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. We have used LabelEncoding technique to encode binary categorical variables, such as Gender,Driving_License,Vehicle_Damage,Previously_Insured all this columns.\n",
        "2. We have used LabelEncoding for Vehicle_Age column also, here we have 3 various categories which is our ordinal categorical variable that's why we have used LabelEncoding in this case also.\n",
        "3. For Region Code and Policy_Sales_Channel code we have just converted this features data types from float to integer."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "t9Qt8jyxJ3oX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(3)"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop('Driving_License',axis=1)"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. We have dropped Driving_License column from our dataset, as we have checked for value counts almost 99.9 % records has same value hence we have dropped this column.\n",
        "2. We have checked in hypothesis testing for some of the features which are there in above dataframe, Annual Pemium, Region code this columns are affecting our response directly, so we need to keep this columns in our dataset.\n",
        "3. We have checked for vintage and Vehicle damage columns also, after doing hypothesis testing we could say this columns also affecting our dependant variable indirectly, hence we need to keep this columns.\n",
        "4. This is how we have checked for all the features and we are keeping all necessary columns in the dataset after removing few columns which were doesn't really helpful. "
      ],
      "metadata": {
        "id": "k8WDDMUoKsE8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(3)"
      ],
      "metadata": {
        "id": "alOEda6PKoAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Annaul Premium column has highly positively skewed data, so let's transform this feature to bring data points closer by doing sqrt transformation."
      ],
      "metadata": {
        "id": "gUYN1fEJX4Tr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Annual_Premium_sqrt'] = np.sqrt(df['Annual_Premium'])\n",
        "df.drop('Annual_Premium',axis=1,inplace=True)\n",
        "df.head(3)"
      ],
      "metadata": {
        "id": "56yhhe_rXkAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Now In our dataset we have left with three contineous columns Age,Vintage,Annual_Premium_sqrt.\n",
        "- Out of that Age and Vintage both this columns have normal distribution and we have already transformed Annual Premium column by using square root transformation.\n",
        "- All our data points are closer to each other, still let's perform standard scaling on our data points.\n"
      ],
      "metadata": {
        "id": "m8RqU0trZb0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfnew = df.copy()\n",
        "X = df.drop('Response',axis=1)\n",
        "y = df[['Response']]\n",
        "sc = StandardScaler() \n",
        "X_sc = sc.fit_transform(X)\n",
        "X_sc = pd.DataFrame(X_sc,columns=[X.columns])"
      ],
      "metadata": {
        "id": "DxRlC0I4ayhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfnew.head(3)"
      ],
      "metadata": {
        "id": "F27GFKhVbNXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_sc.head(3)"
      ],
      "metadata": {
        "id": "b8ZdGA_irWl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Mulicollenearity check 2 (VIF Method)**\n",
        "- VIF stands for varaiance inflation factor\n",
        "- In VIF we regress every independant variable with each other and find the r-square\n",
        "- After finding r-square we use below VIF formula to find VIF index\n",
        "VIF index = 1/1-r.square\n",
        "- If VIF is more than 5 then we say that multi-collinearity exists"
      ],
      "metadata": {
        "id": "A83BCMNbgkjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vif(ind_var):\n",
        "    from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "    result_df = pd.DataFrame() \n",
        "    result_df[\"Feature\"] = ind_var.columns\n",
        "    result_df[\"vif\"] = [variance_inflation_factor(ind_var.values,i)for i in range(ind_var.shape[1])]\n",
        "    return result_df"
      ],
      "metadata": {
        "id": "5LpRF7tZhFMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop('Response',axis=1)"
      ],
      "metadata": {
        "id": "-oLhsAWxe0so"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vif(X)"
      ],
      "metadata": {
        "id": "ZQGFhV1RhTwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- From above VIF table we could say that there is multicollinearity present in the dataset.Some features are depenant on each other such as Annual premium and Age, so we need to take care of this things by doing dimensionallity reduction.\n",
        "- Let's do Principle component analysis to reduce dimensionallity in the dataset."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Principle Component Analysis**"
      ],
      "metadata": {
        "id": "xv08BY52ggP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = dfnew.drop('Response',axis=1)\n",
        "y = dfnew[['Response']]"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Let's perform PCA on our independant variables"
      ],
      "metadata": {
        "id": "GHZQj0ImhECR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_pca(x): \n",
        "    \n",
        "    col = []\n",
        "    n_comp = len(x.columns)\n",
        "    \n",
        "    # step1: applying standerd scaler\n",
        "    \n",
        "    x = StandardScaler().fit_transform(x) \n",
        "    \n",
        "    # step2: applying PCA\n",
        "    \n",
        "    for i in range(1,n_comp):\n",
        "        pca = PCA(n_components = i)\n",
        "        p_components = pca.fit_transform(x) \n",
        "        \n",
        "        evr = np.cumsum(pca.explained_variance_ratio_)\n",
        "        \n",
        "        if evr[i-1] > 0.9:\n",
        "            n_components = i \n",
        "            break\n",
        "            \n",
        "    ## creating the DataFrame\n",
        "    \n",
        "    for j in range(1,n_components+1):\n",
        "        col.append(\"PC_\"+str(j))\n",
        "        \n",
        "    result_df = pd.DataFrame(p_components,columns=col)\n",
        "    \n",
        "    return result_df"
      ],
      "metadata": {
        "id": "EYH5JwNBhCoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_pca = apply_pca(X_sc)"
      ],
      "metadata": {
        "id": "1CtMIEiYh4Od"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_pca.head(3)"
      ],
      "metadata": {
        "id": "CDjXoZtth_oS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### As we could see above dataset is heavily imbalanced and around 80 % of records belongs to 'No' category and only 20% records belongs to 'Yes' category hence we need balance out the data, so that our model couldn't be biased to a single variable."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Synthetic Minority Oversampling Technique (SMOTE): \n",
        "We are going to use this technique to balance out our dataset.This technique generates synthetic data for the minority class.\n",
        "\n",
        "SMOTE (Synthetic Minority Oversampling Technique) works by randomly picking a point from the minority class and computing the k-nearest neighbors for this point. The synthetic points are added between the chosen point and its neighbors."
      ],
      "metadata": {
        "id": "MSqXrZqDsgZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "smote = SMOTE()\n",
        "\n",
        "# fit predictor and target variable\n",
        "x_smote, y_smote = smote.fit_resample(X_pca,y)\n",
        "\n",
        "print('Original dataset shape', len(X_pca))\n",
        "print('Resampled dataset shape', len(y_smote))"
      ],
      "metadata": {
        "id": "o_mP1uoIsfjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SMOTE algorithm works in 4 simple steps:\n",
        "\n",
        "Choose a minority class as the input vector\n",
        "Find its k nearest neighbors (k_neighbors is specified as an argument in the SMOTE() function)\n",
        "Choose one of these neighbors and place a synthetic point anywhere on the line joining the point under consideration and its chosen neighbor\n",
        "Repeat the steps until data is balanced"
      ],
      "metadata": {
        "id": "VEWcwEXts51T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_smote.value_counts()"
      ],
      "metadata": {
        "id": "QeDWsQtjtbPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We could see above now our dependant variable is balanced out and we can move towards model-building now."
      ],
      "metadata": {
        "id": "uNWOhnoXtnfp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test,y_train,y_test = train_test_split(x_smote,y_smote,test_size=0.2,random_state=43)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are doing 80:20 split for 80% goes in model training and 20% will go in model testing as we have around 4,00,000 records in total which is a moderate sample in size that's why we are using 80:20 split."
      ],
      "metadata": {
        "id": "TiiuewDVotgU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Model Selection:**\n",
        "\n",
        "\n",
        "1. Objective in this project is to build a model which will give good accuracy and apart from that as we are working in insurance domain, so model interpetability and explianability are major concerns in this case.  \n",
        "2. Logistic/Ridge: We will try to achieve good accuracy by using, white box models such as Logistic regression and it's penalty functions such as ridge cv.\n",
        "3. Decision tree could also be one of the option as we will have ample amount of explainibilty if we will be able to deploy decision tree in this case as we have most of the indepenendant varibles in categorical format.\n",
        "4. KNN: That is K-Nearest-Neighbours could also be one of the option, in this case as we could be able to explain output and there are chances that KNN could outperform other models as we are dealing with a dependant variable which has only two categories.\n",
        "4. We are not going to check for black box and grey box models in this case due to above reasons.\n"
      ],
      "metadata": {
        "id": "EcvFINQ-ddmF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Logistic Regression"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' Model training and initial performance '''\n",
        "\n",
        "le = LogisticRegression()\n",
        "le.fit(X_train,y_train)\n",
        "y_pre = le.predict(X_test)\n",
        "\n",
        "print(f'Accuracy Score: {accuracy_score(y_pre,y_test)}')\n",
        "print(f'F1 Score: {f1_score(y_pre,y_test)}')"
      ],
      "metadata": {
        "id": "GSgR5NvYu-FL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Model performance and cross validation'''\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1) \n",
        "lst_accu_stratified = []\n",
        "\n",
        "for train_index, test_index in skf.split(x_smote, y_smote): \n",
        "    X_train_fold, X_test_fold = x_smote.iloc[train_index], x_smote.iloc[test_index] \n",
        "    y_train_fold, y_test_fold = y_smote.iloc[train_index], y_smote.iloc[test_index] \n",
        "    le.fit(X_train_fold, y_train_fold) \n",
        "    lst_accu_stratified.append(le.score(X_test_fold, y_test_fold))\n",
        "\n",
        "print('Maximum Accuracy',max(lst_accu_stratified)) \n",
        "print('Minimum Accuracy:',min(lst_accu_stratified)) \n",
        "print('Overall Accuracy:',np.mean(lst_accu_stratified))"
      ],
      "metadata": {
        "id": "j1kCQWbKt9or"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Hyperparameter tunning'''\n",
        "\n",
        "# grid = {\"solver\" : ['newton-cg', 'lbfgs', 'liblinear'],\"penalty\" : ['l2'],\"C\":[100, 10, 1.0, 0.1, 0.01]}\n",
        "# lr = LogisticRegression()\n",
        "\n",
        "# # define grid search\n",
        "# cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# grid_search = GridSearchCV(estimator=lr,param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n",
        "# grid_result = grid_search.fit(x_smote,y_smote)\n",
        "# # summarize results\n",
        "# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "\n",
        "#Best: 0.784047 using {'C': 0.01, 'penalty': 'l2', 'solver': 'newton-cg'}"
      ],
      "metadata": {
        "id": "p3kpT8zg3gZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Tunned model performance cross validation'''\n",
        "\n",
        "le = LogisticRegression(solver='newton-cg',penalty='l2',C=0.01)\n",
        "\n",
        "lst_accu_stratified = []\n",
        "\n",
        "for train_index, test_index in skf.split(x_smote, y_smote): \n",
        "    X_train_fold, X_test_fold = x_smote.iloc[train_index], x_smote.iloc[test_index] \n",
        "    y_train_fold, y_test_fold = y_smote.iloc[train_index], y_smote.iloc[test_index] \n",
        "    le.fit(X_train_fold, y_train_fold) \n",
        "    lst_accu_stratified.append(le.score(X_test_fold, y_test_fold))\n",
        "\n",
        "print('Maximum Accuracy',max(lst_accu_stratified)) \n",
        "print('Minimum Accuracy:',min(lst_accu_stratified)) \n",
        "print('Overall Accuracy:',np.mean(lst_accu_stratified))"
      ],
      "metadata": {
        "id": "VIRFbSnG3Tq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Ridge Classifier"
      ],
      "metadata": {
        "id": "4sb03_URidiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' Model training and initial performance '''\n",
        "\n",
        "rc = RidgeClassifier()\n",
        "rc.fit(X_train,y_train)\n",
        "y_pre = rc.predict(X_test)\n",
        "\n",
        "print(f'Accuracy Score: {accuracy_score(y_pre,y_test)}')\n",
        "print(f'Recall Score: {f1_score(y_pre,y_test)}')"
      ],
      "metadata": {
        "id": "-qvVZCd7inzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Model performance and cross validation'''\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1) \n",
        "lst_accu_stratified = []\n",
        "\n",
        "for train_index, test_index in skf.split(x_smote, y_smote): \n",
        "    X_train_fold, X_test_fold = x_smote.iloc[train_index], x_smote.iloc[test_index] \n",
        "    y_train_fold, y_test_fold = y_smote.iloc[train_index], y_smote.iloc[test_index] \n",
        "    rc.fit(X_train_fold, y_train_fold) \n",
        "    lst_accu_stratified.append(rc.score(X_test_fold, y_test_fold))\n",
        "\n",
        "print('Maximum Accuracy',max(lst_accu_stratified)) \n",
        "print('Minimum Accuracy:',min(lst_accu_stratified)) \n",
        "print('Overall Accuracy:',np.mean(lst_accu_stratified))"
      ],
      "metadata": {
        "id": "UdJ6EwBImBof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Hyperparameter tunning'''\n",
        "\n",
        "# grid = {\"alpha\": [1e-15, 1e-13,1e-11,1e-9,1e-7,1e-5, 1e-3,0.001,0.01,0.1,0.4,0.8,1.0,3.0,5.0,8.0]}\n",
        "# rc = RidgeClassifier()\n",
        "\n",
        "# # define grid search\n",
        "# cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# grid_search = GridSearchCV(estimator=rc,param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n",
        "# grid_result = grid_search.fit(x_smote,y_smote)\n",
        "# #summarize results\n",
        "# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "\n",
        "#Best: 0.784183 using {'alpha': 1e-15}"
      ],
      "metadata": {
        "id": "DGwu9MMemQ_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Tunned Model performance and cross validation'''\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1) \n",
        "lst_accu_stratified = []\n",
        "rc = RidgeClassifier(alpha=1e-15)\n",
        "\n",
        "for train_index, test_index in skf.split(x_smote, y_smote): \n",
        "    X_train_fold, X_test_fold = x_smote.iloc[train_index], x_smote.iloc[test_index] \n",
        "    y_train_fold, y_test_fold = y_smote.iloc[train_index], y_smote.iloc[test_index] \n",
        "    rc.fit(X_train_fold, y_train_fold) \n",
        "    lst_accu_stratified.append(rc.score(X_test_fold, y_test_fold))\n",
        "\n",
        "print('Maximum Accuracy',max(lst_accu_stratified)) \n",
        "print('Minimum Accuracy:',min(lst_accu_stratified)) \n",
        "print('Overall Accuracy:',np.mean(lst_accu_stratified))"
      ],
      "metadata": {
        "id": "UVGsT1OvnSz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. KNN Classifier"
      ],
      "metadata": {
        "id": "64SKpea7raxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' Model training and initial performance '''\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train,y_train)\n",
        "y_pre = knn.predict(X_test)\n",
        "\n",
        "print(f'Accuracy Score: {accuracy_score(y_pre,y_test)}')\n",
        "print(f'F1 Score: {f1_score(y_pre,y_test)}')"
      ],
      "metadata": {
        "id": "0s5jJyO_pIKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Model performance and cross validation'''\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1) \n",
        "lst_accu_stratified = []\n",
        "\n",
        "for train_index, test_index in skf.split(x_smote, y_smote): \n",
        "    X_train_fold, X_test_fold = x_smote.iloc[train_index], x_smote.iloc[test_index] \n",
        "    y_train_fold, y_test_fold = y_smote.iloc[train_index], y_smote.iloc[test_index] \n",
        "    knn.fit(X_train_fold, y_train_fold) \n",
        "    lst_accu_stratified.append(knn.score(X_test_fold, y_test_fold))\n",
        "\n",
        "print('Maximum Accuracy',max(lst_accu_stratified)) \n",
        "print('Minimum Accuracy:',min(lst_accu_stratified)) \n",
        "print('Overall Accuracy:',np.mean(lst_accu_stratified))"
      ],
      "metadata": {
        "id": "SKH009VzryPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Hyperparameter tunning'''\n",
        "\n",
        "# grid = {\"n_neighbors\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]}\n",
        "# knn = KNeighborsClassifier()\n",
        "\n",
        "# # # define grid search\n",
        "\n",
        "# cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# grid_search = GridSearchCV(estimator=knn,param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n",
        "# grid_result = grid_search.fit(x_smote,y_smote)\n",
        "# # #summarize results\n",
        "# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "\n",
        "# #Best: 0.903341 using {'n_neighbors': 1}"
      ],
      "metadata": {
        "id": "6WRC_BJmsayV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Tunned Model performance and cross validation'''\n",
        "\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1) \n",
        "lst_accu_stratified = []\n",
        "knn = KNeighborsClassifier(n_neighbors=1)\n",
        "\n",
        "for train_index, test_index in skf.split(x_smote, y_smote): \n",
        "    X_train_fold, X_test_fold = x_smote.iloc[train_index], x_smote.iloc[test_index] \n",
        "    y_train_fold, y_test_fold = y_smote.iloc[train_index], y_smote.iloc[test_index] \n",
        "    knn.fit(X_train_fold, y_train_fold) \n",
        "    lst_accu_stratified.append(knn.score(X_test_fold, y_test_fold))\n",
        "\n",
        "print('Maximum Accuracy',max(lst_accu_stratified)) \n",
        "print('Minimum Accuracy:',min(lst_accu_stratified)) \n",
        "print('Overall Accuracy:',np.mean(lst_accu_stratified))"
      ],
      "metadata": {
        "id": "RHN1mqbNttE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Decision Tree Classifier"
      ],
      "metadata": {
        "id": "jaq5lbZkwmha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' Model training and initial performance '''\n",
        "\n",
        "dtc = DecisionTreeClassifier()\n",
        "dtc.fit(X_train,y_train)\n",
        "y_pre = dtc.predict(X_test)\n",
        "\n",
        "print(f'Accuracy Score: {accuracy_score(y_pre,y_test)}')\n",
        "print(f'F1 Score: {f1_score(y_pre,y_test)}')"
      ],
      "metadata": {
        "id": "cytZQJotqJzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Model performance and cross validation'''\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1) \n",
        "lst_accu_stratified = []\n",
        "\n",
        "for train_index, test_index in skf.split(x_smote, y_smote): \n",
        "    X_train_fold, X_test_fold = x_smote.iloc[train_index], x_smote.iloc[test_index] \n",
        "    y_train_fold, y_test_fold = y_smote.iloc[train_index], y_smote.iloc[test_index] \n",
        "    dtc.fit(X_train_fold, y_train_fold) \n",
        "    lst_accu_stratified.append(dtc.score(X_test_fold, y_test_fold))\n",
        "\n",
        "print('Maximum Accuracy',max(lst_accu_stratified)) \n",
        "print('Minimum Accuracy:',min(lst_accu_stratified)) \n",
        "print('Overall Accuracy:',np.mean(lst_accu_stratified))"
      ],
      "metadata": {
        "id": "R9InCSuww6aZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Hyperparameter tunning'''\n",
        "\n",
        "# grid = {'max_depth': [2,3,5,10,20,40,60,100,150,200],'min_samples_leaf': [5,10,20,50,100],'criterion': [\"gini\", \"entropy\"]}\n",
        "# dtc = DecisionTreeClassifier()\n",
        "\n",
        "# # # define grid search\n",
        "\n",
        "# cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# grid_search = GridSearchCV(estimator=dtc,param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n",
        "# grid_result = grid_search.fit(x_smote,y_smote)\n",
        "# # #summarize results\n",
        "# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "\n",
        "#Best: 'max_depth': None,'min_samples_leaf': 1,'criterion': 'gini'"
      ],
      "metadata": {
        "id": "txrvT3rsxPgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Tunned Model performance and cross validation'''\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1) \n",
        "lst_accu_stratified = []\n",
        "\n",
        "dtc = DecisionTreeClassifier(min_samples_leaf=1,criterion=\"gini\")\n",
        "\n",
        "for train_index, test_index in skf.split(x_smote, y_smote): \n",
        "    X_train_fold, X_test_fold = x_smote.iloc[train_index], x_smote.iloc[test_index] \n",
        "    y_train_fold, y_test_fold = y_smote.iloc[train_index], y_smote.iloc[test_index] \n",
        "    dtc.fit(X_train_fold, y_train_fold) \n",
        "    lst_accu_stratified.append(dtc.score(X_test_fold, y_test_fold))\n",
        "\n",
        "print('Maximum Accuracy',max(lst_accu_stratified)) \n",
        "print('Minimum Accuracy:',min(lst_accu_stratified)) \n",
        "print('Overall Accuracy:',np.mean(lst_accu_stratified))"
      ],
      "metadata": {
        "id": "INlQnxEOyfv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = KNeighborsClassifier(n_neighbors=1)\n",
        "model.fit(X_train, y_train)\n",
        "pickle.dump(model, open('model.pkl', 'wb'))\n",
        "pickled_model = pickle.load(open('model.pkl', 'rb'))"
      ],
      "metadata": {
        "id": "xCBGNG8v6pPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pre = pickled_model.predict(X_test)\n",
        "print(f'Recall Score: {recall_score(y_test,y_pre)}')\n",
        "print(f'F1 Score: {f1_score(y_test,y_pre)}')"
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. As we are working in insurance domain that's why, We have used different white box models in this project to predict our dependant variable so that we could have good explainability and interpretability and we are getting decent amount of accuracy for all of the above models.\n",
        "2. In our project we have used distance based algorithm that is K Nearest Neighbour also which is giving highest accuracy followed by decision tree and then logistic regression. We could go with either of it.\n",
        "3. We could increase recall rate of our models by adjusting threshold values but at this time our precision which is highest in above case will also decrease accordingly, so it depends on client requirments completly. \n",
        "4. If our client don't want to miss customers if there is even smaller probabilty of conversion, then we could go with recall rate and by adjusting threshold we could achieve desired recall score.\n",
        "5. If our client want to decrease conversion cost also and if they dont want to miss out covertable customers also then we could go with, f1 score where precision and recall both are given same weightage."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Let's check who is intersted in buying vehicle insurance from us !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}